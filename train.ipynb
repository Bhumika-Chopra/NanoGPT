{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "759fe1da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data length in characters: 1115394\n",
      "Vocabulary size: 65\n"
     ]
    }
   ],
   "source": [
    "with open(\"input.txt\", \"r\") as file:\n",
    "    text = file.read()\n",
    "\n",
    "print(\"Input data length in characters:\", len(text))\n",
    "\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "# print(\"Unique characters:\", chars)\n",
    "print(\"Vocabulary size:\", vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07c94d85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.9.1\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "print(torch.backends.mps.is_available())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0cf69df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# print(\"Using device:\", device)\n",
    "\n",
    "device = 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81bf92d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a mapping from characters to indices and vice versa\n",
    "char_to_idx = {ch: i for i, ch in enumerate(chars)}\n",
    "idx_to_char = {i: ch for i, ch in enumerate(chars)}\n",
    "encode = lambda s: [char_to_idx[c] for c in s]\n",
    "decode = lambda l: ''.join([idx_to_char[i] for i in l])\n",
    "\n",
    "# import tiktoken\n",
    "# tiktokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "# print(tiktokenizer.encode(\"Hello, world!\"))\n",
    "# vocab_size = tiktokenizer.n_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6347c9a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115394]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "# wrap into a torch tensor\n",
    "import torch\n",
    "data = torch.tensor(encode(text), dtype=torch.int64)\n",
    "print(data.shape, data.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b5f6804b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data length in tokens: 1003854\n",
      "Test data length in tokens: 111540\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train, test = train_test_split(data, test_size=0.1, random_state=42)\n",
    "print(\"Train data length in tokens:\", len(train))\n",
    "print(\"Test data length in tokens:\", len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c28a5eb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when input is [47] the target: 58\n",
      "when input is [47, 58] the target: 50\n",
      "when input is [47, 58, 50] the target: 1\n",
      "when input is [47, 58, 50, 1] the target: 46\n",
      "when input is [47, 58, 50, 1, 46] the target: 52\n",
      "when input is [47, 58, 50, 1, 46, 52] the target: 42\n",
      "when input is [47, 58, 50, 1, 46, 52, 42] the target: 1\n",
      "when input is [47, 58, 50, 1, 46, 52, 42, 1] the target: 10\n"
     ]
    }
   ],
   "source": [
    "block_size = 8\n",
    "x = train[:block_size]\n",
    "y = train[1:block_size + 1]\n",
    "for b in range(block_size):\n",
    "    context = x[:b + 1]\n",
    "    target = y[b]\n",
    "    print(f\"when input is {context.tolist()} the target: {target.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e436ec6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs: torch.Size([4, 8])\n",
      "tensor([[58, 46, 56, 57, 59, 32,  0, 58],\n",
      "        [57, 59, 46,  1, 32, 59, 39,  1],\n",
      "        [56,  1, 40, 61, 61,  1, 63,  1],\n",
      "        [ 1, 59, 56, 45,  1, 45, 42, 47]], device='mps:0')\n",
      "targets: torch.Size([4, 8])\n",
      "tensor([[46, 56, 57, 59, 32,  0, 58, 42],\n",
      "        [59, 46,  1, 32, 59, 39,  1, 43],\n",
      "        [ 1, 40, 61, 61,  1, 63,  1, 44],\n",
      "        [59, 56, 45,  1, 45, 42, 47,  1]], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "batch_size = 4\n",
    "block_size = 8\n",
    "\n",
    "def get_batch(split):\n",
    "    data = train if split == 'train' else test\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i: i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1: i+block_size+1] for i in ix])\n",
    "    return x.to(device), y.to(device)\n",
    "\n",
    "xb, yb = get_batch('train')\n",
    "print(\"inputs:\", xb.shape)\n",
    "print(xb)\n",
    "print(\"targets:\", yb.shape)\n",
    "print(yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bb7694a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits shape: torch.Size([32, 32])\n",
      "logits: tensor([[-0.0498, -1.3200,  0.4434,  ..., -0.6524, -0.1091,  0.4586],\n",
      "        [ 0.0875, -0.0524,  0.0993,  ..., -0.0978,  1.0993, -0.4966],\n",
      "        [-0.1554,  0.0685,  0.4702,  ...,  0.0821, -0.8721, -0.2592],\n",
      "        ...,\n",
      "        [ 1.2315,  0.2733, -0.5318,  ...,  0.1619,  0.8393,  0.3330],\n",
      "        [ 0.0587,  0.2208,  0.6501,  ..., -0.8241, -0.6447,  0.1272],\n",
      "        [ 0.4135,  0.0210, -0.2151,  ..., -0.8818,  0.8399, -0.4806]],\n",
      "       device='mps:0', grad_fn=<ViewBackward0>)\n",
      "loss: tensor(1.4265, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "\n",
      ":NAG&3;K?!MC-,EK'EP-QDRM&\n",
      "OOD!KKE& .N$:QQ---E3ECOIHH?J K.:-AQ;CDKM!MRAOAO:D PIHN!A&?!.!OA3QFCJKPL?DL\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size, n_mebed):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_mebed)\n",
    "        self.positional_embedding_table = nn.Embedding(block_size, n_mebed)\n",
    "        self.lm_head = nn.Linear(n_mebed, vocab_size)\n",
    "\n",
    "    def forward(self, batch, target):\n",
    "        B, T = batch.shape\n",
    "\n",
    "        pos = self.positional_embedding_table(torch.arange(T, device=device)) # (T, n_embed)\n",
    "        logits = self.token_embedding_table(batch) # (B, T, n_embed)\n",
    "        x = logits + pos # (B, T, n_embed)\n",
    "        preds = self.lm_head(x) # (B, T, vocab_size)\n",
    "\n",
    "        if target is None:\n",
    "            return preds, None\n",
    "        \n",
    "        B, T, C = preds.shape\n",
    "        preds = preds.view(B*T, C)\n",
    "        target = target.view(B*T)\n",
    "        loss = F.cross_entropy(preds, target)\n",
    "        return preds, loss\n",
    "\n",
    "    def generate(self, batch, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            logits, _ = self(batch, None)\n",
    "            logits = logits[:, -1, :]\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "            batch = torch.cat((batch, next_token), dim=1)\n",
    "        return batch\n",
    "\n",
    "xb, yb = get_batch('train')\n",
    "BLM = BigramLanguageModel(32, vocab_size)\n",
    "BLM.to(device)\n",
    "\n",
    "logits, loss = BLM(xb, yb)\n",
    "print(\"logits shape:\", logits.shape)  # (batch_size, block_size, vocab_size)\n",
    "print(\"logits:\", logits)\n",
    "print(\"loss:\", loss)\n",
    "\n",
    "idx = torch.zeros((1,1), dtype=torch.long, device=device)\n",
    "decoded_text = decode(BLM.generate(idx, max_new_tokens=100)[0].tolist())\n",
    "print(decoded_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e31d3b18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 1.1168, val loss 1.1212\n",
      "step 1000: train loss 0.6392, val loss 0.6203\n",
      "step 2000: train loss 0.6244, val loss 0.6289\n",
      "step 3000: train loss 0.6409, val loss 0.6463\n",
      "step 4000: train loss 0.6206, val loss 0.6236\n",
      "step 5000: train loss 0.6466, val loss 0.6300\n",
      "step 6000: train loss 0.6361, val loss 0.6250\n",
      "step 7000: train loss 0.6328, val loss 0.6183\n",
      "step 8000: train loss 0.6280, val loss 0.6228\n",
      "step 9000: train loss 0.6401, val loss 0.6348\n",
      "loss 0.6672463417053223\n"
     ]
    }
   ],
   "source": [
    "eval_iters = 100\n",
    "optimizer = torch.optim.AdamW(BLM.parameters(), lr=1e-3)\n",
    "\n",
    "# telling pytorch we don't need gradients\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    BLM.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            xb, yb = get_batch(split)\n",
    "            _, loss = BLM(xb, yb)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    BLM.train()\n",
    "    return out\n",
    "\n",
    "batch_size = 32\n",
    "for step in range(10000):\n",
    "    if step % 1000 == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {step}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "    \n",
    "    xb, yb = get_batch('train')\n",
    "    _, loss = BLM(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(f\"loss {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f5699cb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ",P\n",
      "LA  \n",
      " .O;JM.PPD:AH PF\n",
      "S\n",
      " !G$RGBPM!O&:\n",
      "EAJFF3NJI-EDNRL:\n",
      "BH,L?G M!MSO? NCFKBEE,F-DEKFPIB;!\n",
      "!'3:Q-;& D.FK\n",
      "$:KQQONQRF$RQ'DIEK3L-\n",
      "H3S&QD!\n",
      "'&.\n",
      "LF!A'&  B,F??Q3LFJE :,:!:PLNLMS;& SB\n",
      "'?Q'& &BMLDP3B\n",
      "'QD,-P\n",
      "SO;MKQNFM?O\n",
      "\n",
      "DCMMLI\n",
      "\n",
      "JK&E C\n",
      "!P!BO\n",
      " E\n",
      "3OLOEEBELBPJN OHJDH-RQ& !,KRI',MNK&!QHBPF'&M\n",
      "D!P\n",
      "GKF-DA'QGAH$3J;F,I';L!F&!COHJDDJGBDDD,C AIES&;!SSLC'G\n",
      "--$!QD .'&DFCOKQE:ABCH$,;NQEEIOA:EBGML,HKGF3$O'KHFK.F R&IS:FKQ.DSBF:D.OBN?G!A!H.\n",
      "&AP\n",
      "D OA-.F3& \n",
      "KMCC-K&.'DD.& MCG'NQA!'\n",
      "HRGI-:.PCG .GBMHAPMRD$'ICF:?RAH3:BOC;ELKS\n"
     ]
    }
   ],
   "source": [
    "idx = torch.zeros((1,1), dtype=torch.long, device=device)\n",
    "decoded_text = decode(BLM.generate(idx, max_new_tokens=500)[0].tolist())\n",
    "print(decoded_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21603546",
   "metadata": {},
   "source": [
    "### Self Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dfa8676c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1.]])\n",
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
      "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
      "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "B, T, C = 4, 8, 32\n",
    "x = torch.randn(B, T, C)\n",
    "\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "print(tril)\n",
    "weights = torch.zeros((T, T))\n",
    "weights = weights.masked_fill(tril == 0, float('-inf'))\n",
    "weights = F.softmax(weights, dim=1)\n",
    "print(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "50ec8c89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.0248, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0052, 0.0091, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0521, 0.0135, 0.2482, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.3171, 0.0214, 0.1642, 0.1188, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0412, 0.0487, 0.1046, 0.0742, 0.2000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.1060, 0.5347, 0.2059, 0.1030, 0.7402, 0.0192, 0.0000, 0.0000],\n",
      "         [0.4298, 0.3409, 0.1769, 0.2027, 0.0480, 0.8472, 0.2329, 0.0000],\n",
      "         [0.0238, 0.0316, 0.1002, 0.5013, 0.0117, 0.1336, 0.7671, 1.0000]],\n",
      "\n",
      "        [[0.0443, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0042, 0.0375, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0560, 0.0210, 0.2496, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.3679, 0.1441, 0.4929, 0.0438, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0088, 0.1052, 0.0604, 0.5847, 0.2046, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0367, 0.0895, 0.0362, 0.2074, 0.1029, 0.0115, 0.0000, 0.0000],\n",
      "         [0.0480, 0.5010, 0.0172, 0.1434, 0.2807, 0.7090, 0.7318, 0.0000],\n",
      "         [0.4341, 0.1018, 0.1437, 0.0206, 0.4118, 0.2794, 0.2682, 1.0000]],\n",
      "\n",
      "        [[0.0419, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0679, 0.0901, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0119, 0.0392, 0.1158, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0041, 0.5063, 0.1163, 0.1399, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.7491, 0.0460, 0.2084, 0.0659, 0.0292, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0583, 0.1241, 0.2200, 0.0712, 0.2419, 0.1883, 0.0000, 0.0000],\n",
      "         [0.0107, 0.1200, 0.2721, 0.6404, 0.5979, 0.7420, 0.9713, 0.0000],\n",
      "         [0.0562, 0.0744, 0.0674, 0.0826, 0.1310, 0.0697, 0.0287, 1.0000]],\n",
      "\n",
      "        [[0.2196, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0937, 0.0126, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0881, 0.0591, 0.0066, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0682, 0.0118, 0.0908, 0.0115, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0934, 0.0551, 0.0891, 0.1162, 0.0787, 0.0000, 0.0000, 0.0000],\n",
      "         [0.3185, 0.6763, 0.0329, 0.3541, 0.3450, 0.1410, 0.0000, 0.0000],\n",
      "         [0.0340, 0.0079, 0.3160, 0.0306, 0.0840, 0.6004, 0.1996, 0.0000],\n",
      "         [0.0846, 0.1772, 0.4646, 0.4876, 0.4922, 0.2586, 0.8004, 1.0000]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "torch.Size([4, 8, 16])\n"
     ]
    }
   ],
   "source": [
    "# single attention head\n",
    "# query and key vectors\n",
    "head_size = 16  # hyperparameter\n",
    "key = nn.Linear(C, head_size, bias=False)\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "value = nn.Linear(C, head_size, bias=False)\n",
    "k = key(x)   # (B, T, head_size)\n",
    "q = query(x) # (B, T, head_size)\n",
    "\n",
    "wei = q @ k.transpose(-2, -1) # (B, T, T)\n",
    "\n",
    "\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "wei = F.softmax(wei, dim=1)\n",
    "print(wei)\n",
    "\n",
    "v = value(x) # (B, T, head_size)\n",
    "out = wei @ v # (B, T, head_size)\n",
    "print(out.shape)  # (B, T, head_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3fac4aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    def __init__(self, input_size, head_size = 16, block_size = 8):\n",
    "        super().__init__()\n",
    "        self.head_size = head_size\n",
    "        self.key = nn.Linear(input_size, head_size, bias=False)\n",
    "        self.query = nn.Linear(input_size, head_size, bias=False)\n",
    "        self.value = nn.Linear(input_size, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x)   # (B, T, head_size)\n",
    "        q = self.query(x) # (B, T, head_size)\n",
    "\n",
    "        wei = q @ k.transpose(-2, -1) * self.head_size**-0.5 # (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril == 0, float('-inf'))\n",
    "        wei = F.softmax(wei, dim=1)\n",
    "        \n",
    "        v = self.value(x) # (B, T, head_size)\n",
    "        out = wei @ v # (B, T, head_size)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a0781276",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits shape: torch.Size([256, 65])\n",
      "logits: tensor([[-0.0143, -0.0991, -0.6386,  ..., -0.1259,  0.4938, -0.2159],\n",
      "        [-0.1929,  0.0343, -0.3807,  ..., -0.1594,  0.2260, -0.1756],\n",
      "        [-0.1592, -0.1227, -0.2956,  ..., -0.2066, -0.1374, -0.1083],\n",
      "        ...,\n",
      "        [-0.0792,  0.0890, -0.2257,  ..., -0.2764, -0.1726,  0.0619],\n",
      "        [-0.2901, -0.1173, -0.3509,  ..., -0.2811, -0.1455,  0.0287],\n",
      "        [-0.1252,  0.0076, -0.2314,  ..., -0.2811, -0.1033, -0.0159]],\n",
      "       device='mps:0', grad_fn=<ViewBackward0>)\n",
      "loss: tensor(4.1650, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "DL\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size, n_mebed):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_mebed)\n",
    "        self.positional_embedding_table = nn.Embedding(block_size, n_mebed)\n",
    "        # ensure the self-attention head outputs the same embedding dimension\n",
    "        self.sa_head = Head(n_mebed, n_mebed, 1)\n",
    "        self.lm_head = nn.Linear(n_mebed, vocab_size)\n",
    "\n",
    "    def forward(self, batch, target):\n",
    "        B, T = batch.shape\n",
    "\n",
    "        pos = self.positional_embedding_table(torch.arange(T, device=device)) # (T, n_embed)\n",
    "        logits = self.token_embedding_table(batch) # (B, T, n_embed)\n",
    "        x = logits + pos # (B, T, n_embed)\n",
    "        x = self.sa_head(x) # (B, T, n_embed)\n",
    "        preds = self.lm_head(x) # (B, T, vocab_size)\n",
    "\n",
    "        if target is None:\n",
    "            return preds, None\n",
    "        \n",
    "        B, T, C = preds.shape\n",
    "        preds = preds.view(B*T, C)\n",
    "        target = target.view(B*T)\n",
    "        loss = F.cross_entropy(preds, target)\n",
    "        return preds, loss\n",
    "\n",
    "    def generate(self, batch, max_new_tokens):\n",
    "        B, T = batch.shape\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop if batch size exceeds block_size\n",
    "            batch = batch[:, -T:]\n",
    "            logits, _ = self(batch, None)\n",
    "            logits = logits[:, -1, :]\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "            batch = torch.cat((batch, next_token), dim=1)\n",
    "        return batch\n",
    "\n",
    "xb, yb = get_batch('train')\n",
    "BLM = BigramLanguageModel(vocab_size, 32)\n",
    "BLM.to(device)\n",
    "\n",
    "logits, loss = BLM(xb, yb)\n",
    "print(\"logits shape:\", logits.shape)  # (batch_size, block_size, vocab_size)\n",
    "print(\"logits:\", logits)\n",
    "print(\"loss:\", loss)\n",
    "\n",
    "idx = torch.zeros((1,1), dtype=torch.long, device=device)\n",
    "decoded_text = decode(BLM.generate(idx, max_new_tokens=100)[0].tolist())\n",
    "print(decoded_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dea8efe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 4.1737, val loss 4.1753\n",
      "step 1000: train loss 0.4393, val loss 0.4405\n",
      "step 2000: train loss 0.4242, val loss 0.4205\n",
      "step 3000: train loss 0.4187, val loss 0.4228\n",
      "step 4000: train loss 0.4161, val loss 0.4166\n",
      "step 5000: train loss 0.4182, val loss 0.4160\n",
      "step 6000: train loss 0.4199, val loss 0.4198\n",
      "step 7000: train loss 0.4174, val loss 0.4095\n",
      "step 8000: train loss 0.4148, val loss 0.4196\n",
      "step 9000: train loss 0.4172, val loss 0.4178\n",
      "loss 0.42248502373695374\n"
     ]
    }
   ],
   "source": [
    "BLM = BigramLanguageModel(vocab_size, 32)\n",
    "BLM.to(device)\n",
    "eval_iters = 100\n",
    "optimizer = torch.optim.AdamW(BLM.parameters(), lr=1e-3)\n",
    "\n",
    "# telling pytorch we don't need gradients\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    BLM.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            xb, yb = get_batch(split)\n",
    "            _, loss = BLM(xb, yb)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    BLM.train()\n",
    "    return out\n",
    "\n",
    "batch_size = 32\n",
    "for step in range(10000):\n",
    "    if step % 1000 == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {step}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "    \n",
    "    xb, yb = get_batch('train')\n",
    "    _, loss = BLM(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(f\"loss {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e8d8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, input_size, head_size, num_heads, block_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(input_size, head_size, block_size) for _ in range(num_heads)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        return out"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
